{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jeTrqU1qY6B"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcNlLmbRxiQj"
      },
      "source": [
        "iris = sklearn.datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fk78M4_xrCY"
      },
      "source": [
        "X = np.array(iris.data) #Train data\n",
        "Y = np.array(iris.target) #Train labels\n",
        "num_features = X.data.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTjMJzIT14q6"
      },
      "source": [
        "def calculate(data):\n",
        "\tnum = len(data)\n",
        "\ttotal = [0] * data.shape[1]\n",
        "\tfor sample in data:\n",
        "\t\ttotal = total + sample\n",
        "\tmean = np.divide(total, num)\n",
        "\ttotal = [0] * data.shape[1]\n",
        "\tfor sample in data:\n",
        "\t\ttotal = total + np.square(sample - mean)\n",
        "\tfeatures = np.divide(total, num)\n",
        "\tvar = features ** 2\n",
        "\treturn mean, var\n",
        "\n",
        "def normalize(data):\n",
        "\tmean, var = calculate(data)\n",
        "\tfeatures = np.sqrt(var)\n",
        "\tfor i, sample in enumerate(data):\n",
        "\t\tdata[i] = np.divide((sample - mean), features) \n",
        "\treturn data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaubrwYS2Zha"
      },
      "source": [
        "X = normalize(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e25C_P-4yHcy"
      },
      "source": [
        "clf = MLPClassifier(solver = 'sgd', learning_rate_init = 0.01, learning_rate = 'constant', hidden_layer_sizes = (15, 10, 5), max_iter = 400, verbose = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQZUBfPhqr8R",
        "outputId": "36177d7e-a0e2-4469-e141-b3991ab8fe39"
      },
      "source": [
        "clf.fit(X, Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.20026748\n",
            "Iteration 2, loss = 1.19541543\n",
            "Iteration 3, loss = 1.18892046\n",
            "Iteration 4, loss = 1.18147493\n",
            "Iteration 5, loss = 1.17346837\n",
            "Iteration 6, loss = 1.16473960\n",
            "Iteration 7, loss = 1.15546302\n",
            "Iteration 8, loss = 1.14526224\n",
            "Iteration 9, loss = 1.13343578\n",
            "Iteration 10, loss = 1.11945079\n",
            "Iteration 11, loss = 1.10122596\n",
            "Iteration 12, loss = 1.07820061\n",
            "Iteration 13, loss = 1.05185541\n",
            "Iteration 14, loss = 1.02383267\n",
            "Iteration 15, loss = 0.99574697\n",
            "Iteration 16, loss = 0.96762857\n",
            "Iteration 17, loss = 0.93984582\n",
            "Iteration 18, loss = 0.91264014\n",
            "Iteration 19, loss = 0.88591150\n",
            "Iteration 20, loss = 0.86008272\n",
            "Iteration 21, loss = 0.83502757\n",
            "Iteration 22, loss = 0.81103235\n",
            "Iteration 23, loss = 0.78807665\n",
            "Iteration 24, loss = 0.76656377\n",
            "Iteration 25, loss = 0.74613415\n",
            "Iteration 26, loss = 0.72696756\n",
            "Iteration 27, loss = 0.70882187\n",
            "Iteration 28, loss = 0.69149527\n",
            "Iteration 29, loss = 0.67510180\n",
            "Iteration 30, loss = 0.65942115\n",
            "Iteration 31, loss = 0.64422385\n",
            "Iteration 32, loss = 0.62945783\n",
            "Iteration 33, loss = 0.61540409\n",
            "Iteration 34, loss = 0.60207184\n",
            "Iteration 35, loss = 0.58921158\n",
            "Iteration 36, loss = 0.57711180\n",
            "Iteration 37, loss = 0.56568033\n",
            "Iteration 38, loss = 0.55485091\n",
            "Iteration 39, loss = 0.54462516\n",
            "Iteration 40, loss = 0.53495309\n",
            "Iteration 41, loss = 0.52574445\n",
            "Iteration 42, loss = 0.51735841\n",
            "Iteration 43, loss = 0.50959537\n",
            "Iteration 44, loss = 0.50222029\n",
            "Iteration 45, loss = 0.49535244\n",
            "Iteration 46, loss = 0.48910734\n",
            "Iteration 47, loss = 0.48336883\n",
            "Iteration 48, loss = 0.47793756\n",
            "Iteration 49, loss = 0.47281216\n",
            "Iteration 50, loss = 0.46799324\n",
            "Iteration 51, loss = 0.46346617\n",
            "Iteration 52, loss = 0.45912123\n",
            "Iteration 53, loss = 0.45479359\n",
            "Iteration 54, loss = 0.45057858\n",
            "Iteration 55, loss = 0.44648099\n",
            "Iteration 56, loss = 0.44249249\n",
            "Iteration 57, loss = 0.43855019\n",
            "Iteration 58, loss = 0.43458266\n",
            "Iteration 59, loss = 0.43055770\n",
            "Iteration 60, loss = 0.42655726\n",
            "Iteration 61, loss = 0.42250938\n",
            "Iteration 62, loss = 0.41831160\n",
            "Iteration 63, loss = 0.41405447\n",
            "Iteration 64, loss = 0.40976133\n",
            "Iteration 65, loss = 0.40551063\n",
            "Iteration 66, loss = 0.40129472\n",
            "Iteration 67, loss = 0.39708686\n",
            "Iteration 68, loss = 0.39287332\n",
            "Iteration 69, loss = 0.38866270\n",
            "Iteration 70, loss = 0.38445225\n",
            "Iteration 71, loss = 0.38023293\n",
            "Iteration 72, loss = 0.37601150\n",
            "Iteration 73, loss = 0.37180683\n",
            "Iteration 74, loss = 0.36761650\n",
            "Iteration 75, loss = 0.36341628\n",
            "Iteration 76, loss = 0.35922028\n",
            "Iteration 77, loss = 0.35500853\n",
            "Iteration 78, loss = 0.35082901\n",
            "Iteration 79, loss = 0.34668968\n",
            "Iteration 80, loss = 0.34258966\n",
            "Iteration 81, loss = 0.33851885\n",
            "Iteration 82, loss = 0.33448592\n",
            "Iteration 83, loss = 0.33049457\n",
            "Iteration 84, loss = 0.32653402\n",
            "Iteration 85, loss = 0.32259370\n",
            "Iteration 86, loss = 0.31869824\n",
            "Iteration 87, loss = 0.31484899\n",
            "Iteration 88, loss = 0.31103454\n",
            "Iteration 89, loss = 0.30729402\n",
            "Iteration 90, loss = 0.30360493\n",
            "Iteration 91, loss = 0.29995341\n",
            "Iteration 92, loss = 0.29630209\n",
            "Iteration 93, loss = 0.29260021\n",
            "Iteration 94, loss = 0.28894540\n",
            "Iteration 95, loss = 0.28531126\n",
            "Iteration 96, loss = 0.28167703\n",
            "Iteration 97, loss = 0.27804145\n",
            "Iteration 98, loss = 0.27437282\n",
            "Iteration 99, loss = 0.27070315\n",
            "Iteration 100, loss = 0.26703186\n",
            "Iteration 101, loss = 0.26335994\n",
            "Iteration 102, loss = 0.25968896\n",
            "Iteration 103, loss = 0.25603632\n",
            "Iteration 104, loss = 0.25239097\n",
            "Iteration 105, loss = 0.24875606\n",
            "Iteration 106, loss = 0.24513016\n",
            "Iteration 107, loss = 0.24145469\n",
            "Iteration 108, loss = 0.23784262\n",
            "Iteration 109, loss = 0.23423417\n",
            "Iteration 110, loss = 0.23068011\n",
            "Iteration 111, loss = 0.22714892\n",
            "Iteration 112, loss = 0.22359589\n",
            "Iteration 113, loss = 0.22003052\n",
            "Iteration 114, loss = 0.21648157\n",
            "Iteration 115, loss = 0.21295406\n",
            "Iteration 116, loss = 0.20945704\n",
            "Iteration 117, loss = 0.20598916\n",
            "Iteration 118, loss = 0.20255685\n",
            "Iteration 119, loss = 0.19918240\n",
            "Iteration 120, loss = 0.19583808\n",
            "Iteration 121, loss = 0.19252095\n",
            "Iteration 122, loss = 0.18925875\n",
            "Iteration 123, loss = 0.18605401\n",
            "Iteration 124, loss = 0.18289634\n",
            "Iteration 125, loss = 0.17979758\n",
            "Iteration 126, loss = 0.17675205\n",
            "Iteration 127, loss = 0.17373943\n",
            "Iteration 128, loss = 0.17078368\n",
            "Iteration 129, loss = 0.16789296\n",
            "Iteration 130, loss = 0.16506570\n",
            "Iteration 131, loss = 0.16230222\n",
            "Iteration 132, loss = 0.15959385\n",
            "Iteration 133, loss = 0.15689812\n",
            "Iteration 134, loss = 0.15425597\n",
            "Iteration 135, loss = 0.15166812\n",
            "Iteration 136, loss = 0.14914291\n",
            "Iteration 137, loss = 0.14667129\n",
            "Iteration 138, loss = 0.14426458\n",
            "Iteration 139, loss = 0.14193825\n",
            "Iteration 140, loss = 0.13967299\n",
            "Iteration 141, loss = 0.13747071\n",
            "Iteration 142, loss = 0.13533740\n",
            "Iteration 143, loss = 0.13327944\n",
            "Iteration 144, loss = 0.13131520\n",
            "Iteration 145, loss = 0.12940902\n",
            "Iteration 146, loss = 0.12756013\n",
            "Iteration 147, loss = 0.12576945\n",
            "Iteration 148, loss = 0.12403746\n",
            "Iteration 149, loss = 0.12235622\n",
            "Iteration 150, loss = 0.12072714\n",
            "Iteration 151, loss = 0.11915730\n",
            "Iteration 152, loss = 0.11763829\n",
            "Iteration 153, loss = 0.11617465\n",
            "Iteration 154, loss = 0.11477266\n",
            "Iteration 155, loss = 0.11346225\n",
            "Iteration 156, loss = 0.11219770\n",
            "Iteration 157, loss = 0.11099208\n",
            "Iteration 158, loss = 0.10983310\n",
            "Iteration 159, loss = 0.10871154\n",
            "Iteration 160, loss = 0.10762193\n",
            "Iteration 161, loss = 0.10657151\n",
            "Iteration 162, loss = 0.10556195\n",
            "Iteration 163, loss = 0.10460037\n",
            "Iteration 164, loss = 0.10367573\n",
            "Iteration 165, loss = 0.10278241\n",
            "Iteration 166, loss = 0.10190899\n",
            "Iteration 167, loss = 0.10105786\n",
            "Iteration 168, loss = 0.10023375\n",
            "Iteration 169, loss = 0.09943554\n",
            "Iteration 170, loss = 0.09866315\n",
            "Iteration 171, loss = 0.09791574\n",
            "Iteration 172, loss = 0.09719259\n",
            "Iteration 173, loss = 0.09649248\n",
            "Iteration 174, loss = 0.09581346\n",
            "Iteration 175, loss = 0.09515509\n",
            "Iteration 176, loss = 0.09451571\n",
            "Iteration 177, loss = 0.09389636\n",
            "Iteration 178, loss = 0.09329655\n",
            "Iteration 179, loss = 0.09271493\n",
            "Iteration 180, loss = 0.09214987\n",
            "Iteration 181, loss = 0.09160299\n",
            "Iteration 182, loss = 0.09107680\n",
            "Iteration 183, loss = 0.09056604\n",
            "Iteration 184, loss = 0.09007056\n",
            "Iteration 185, loss = 0.08959052\n",
            "Iteration 186, loss = 0.08913526\n",
            "Iteration 187, loss = 0.08869732\n",
            "Iteration 188, loss = 0.08827149\n",
            "Iteration 189, loss = 0.08785728\n",
            "Iteration 190, loss = 0.08745441\n",
            "Iteration 191, loss = 0.08706201\n",
            "Iteration 192, loss = 0.08667958\n",
            "Iteration 193, loss = 0.08630659\n",
            "Iteration 194, loss = 0.08594290\n",
            "Iteration 195, loss = 0.08558763\n",
            "Iteration 196, loss = 0.08524079\n",
            "Iteration 197, loss = 0.08490236\n",
            "Iteration 198, loss = 0.08457324\n",
            "Iteration 199, loss = 0.08425784\n",
            "Iteration 200, loss = 0.08393908\n",
            "Iteration 201, loss = 0.08361449\n",
            "Iteration 202, loss = 0.08329365\n",
            "Iteration 203, loss = 0.08297714\n",
            "Iteration 204, loss = 0.08266469\n",
            "Iteration 205, loss = 0.08235667\n",
            "Iteration 206, loss = 0.08205345\n",
            "Iteration 207, loss = 0.08175485\n",
            "Iteration 208, loss = 0.08146955\n",
            "Iteration 209, loss = 0.08119512\n",
            "Iteration 210, loss = 0.08092548\n",
            "Iteration 211, loss = 0.08066041\n",
            "Iteration 212, loss = 0.08039950\n",
            "Iteration 213, loss = 0.08014247\n",
            "Iteration 214, loss = 0.07989171\n",
            "Iteration 215, loss = 0.07964722\n",
            "Iteration 216, loss = 0.07940597\n",
            "Iteration 217, loss = 0.07916781\n",
            "Iteration 218, loss = 0.07893151\n",
            "Iteration 219, loss = 0.07869865\n",
            "Iteration 220, loss = 0.07846872\n",
            "Iteration 221, loss = 0.07824171\n",
            "Iteration 222, loss = 0.07801756\n",
            "Iteration 223, loss = 0.07779626\n",
            "Iteration 224, loss = 0.07757797\n",
            "Iteration 225, loss = 0.07736210\n",
            "Iteration 226, loss = 0.07714872\n",
            "Iteration 227, loss = 0.07693772\n",
            "Iteration 228, loss = 0.07672901\n",
            "Iteration 229, loss = 0.07652251\n",
            "Iteration 230, loss = 0.07631822\n",
            "Iteration 231, loss = 0.07611612\n",
            "Iteration 232, loss = 0.07591613\n",
            "Iteration 233, loss = 0.07571796\n",
            "Iteration 234, loss = 0.07552162\n",
            "Iteration 235, loss = 0.07532704\n",
            "Iteration 236, loss = 0.07513419\n",
            "Iteration 237, loss = 0.07494301\n",
            "Iteration 238, loss = 0.07475346\n",
            "Iteration 239, loss = 0.07456555\n",
            "Iteration 240, loss = 0.07437941\n",
            "Iteration 241, loss = 0.07419488\n",
            "Iteration 242, loss = 0.07401177\n",
            "Iteration 243, loss = 0.07383010\n",
            "Iteration 244, loss = 0.07364982\n",
            "Iteration 245, loss = 0.07347091\n",
            "Iteration 246, loss = 0.07329340\n",
            "Iteration 247, loss = 0.07311720\n",
            "Iteration 248, loss = 0.07294227\n",
            "Iteration 249, loss = 0.07276863\n",
            "Iteration 250, loss = 0.07259744\n",
            "Iteration 251, loss = 0.07242678\n",
            "Iteration 252, loss = 0.07225744\n",
            "Iteration 253, loss = 0.07208913\n",
            "Iteration 254, loss = 0.07192181\n",
            "Iteration 255, loss = 0.07175551\n",
            "Iteration 256, loss = 0.07159019\n",
            "Iteration 257, loss = 0.07142583\n",
            "Iteration 258, loss = 0.07126242\n",
            "Iteration 259, loss = 0.07109992\n",
            "Iteration 260, loss = 0.07091665\n",
            "Iteration 261, loss = 0.07071434\n",
            "Iteration 262, loss = 0.07050421\n",
            "Iteration 263, loss = 0.07028825\n",
            "Iteration 264, loss = 0.07006784\n",
            "Iteration 265, loss = 0.06984355\n",
            "Iteration 266, loss = 0.06961522\n",
            "Iteration 267, loss = 0.06938375\n",
            "Iteration 268, loss = 0.06914961\n",
            "Iteration 269, loss = 0.06891311\n",
            "Iteration 270, loss = 0.06867460\n",
            "Iteration 271, loss = 0.06843321\n",
            "Iteration 272, loss = 0.06818493\n",
            "Iteration 273, loss = 0.06793376\n",
            "Iteration 274, loss = 0.06768053\n",
            "Iteration 275, loss = 0.06742600\n",
            "Iteration 276, loss = 0.06717996\n",
            "Iteration 277, loss = 0.06693934\n",
            "Iteration 278, loss = 0.06669731\n",
            "Iteration 279, loss = 0.06645325\n",
            "Iteration 280, loss = 0.06620381\n",
            "Iteration 281, loss = 0.06595316\n",
            "Iteration 282, loss = 0.06570631\n",
            "Iteration 283, loss = 0.06555934\n",
            "Iteration 284, loss = 0.06541645\n",
            "Iteration 285, loss = 0.06527588\n",
            "Iteration 286, loss = 0.06513571\n",
            "Iteration 287, loss = 0.06499733\n",
            "Iteration 288, loss = 0.06486014\n",
            "Iteration 289, loss = 0.06472405\n",
            "Iteration 290, loss = 0.06458907\n",
            "Iteration 291, loss = 0.06445523\n",
            "Iteration 292, loss = 0.06432231\n",
            "Iteration 293, loss = 0.06419032\n",
            "Iteration 294, loss = 0.06405915\n",
            "Iteration 295, loss = 0.06392870\n",
            "Iteration 296, loss = 0.06379900\n",
            "Iteration 297, loss = 0.06366992\n",
            "Iteration 298, loss = 0.06354150\n",
            "Iteration 299, loss = 0.06341373\n",
            "Iteration 300, loss = 0.06329256\n",
            "Iteration 301, loss = 0.06316909\n",
            "Iteration 302, loss = 0.06304507\n",
            "Iteration 303, loss = 0.06292257\n",
            "Iteration 304, loss = 0.06280001\n",
            "Iteration 305, loss = 0.06268246\n",
            "Iteration 306, loss = 0.06256390\n",
            "Iteration 307, loss = 0.06244537\n",
            "Iteration 308, loss = 0.06232689\n",
            "Iteration 309, loss = 0.06221007\n",
            "Iteration 310, loss = 0.06209310\n",
            "Iteration 311, loss = 0.06197890\n",
            "Iteration 312, loss = 0.06186334\n",
            "Iteration 313, loss = 0.06174927\n",
            "Iteration 314, loss = 0.06163493\n",
            "Iteration 315, loss = 0.06152275\n",
            "Iteration 316, loss = 0.06140920\n",
            "Iteration 317, loss = 0.06129801\n",
            "Iteration 318, loss = 0.06118590\n",
            "Iteration 319, loss = 0.06107533\n",
            "Iteration 320, loss = 0.06096507\n",
            "Iteration 321, loss = 0.06085464\n",
            "Iteration 322, loss = 0.06074645\n",
            "Iteration 323, loss = 0.06063661\n",
            "Iteration 324, loss = 0.06052922\n",
            "Iteration 325, loss = 0.06042025\n",
            "Iteration 326, loss = 0.06031380\n",
            "Iteration 327, loss = 0.06020633\n",
            "Iteration 328, loss = 0.06010043\n",
            "Iteration 329, loss = 0.05999395\n",
            "Iteration 330, loss = 0.05988973\n",
            "Iteration 331, loss = 0.05978364\n",
            "Iteration 332, loss = 0.05968078\n",
            "Iteration 333, loss = 0.05957603\n",
            "Iteration 334, loss = 0.05947172\n",
            "Iteration 335, loss = 0.05936885\n",
            "Iteration 336, loss = 0.05926593\n",
            "Iteration 337, loss = 0.05916334\n",
            "Iteration 338, loss = 0.05906164\n",
            "Iteration 339, loss = 0.05895941\n",
            "Iteration 340, loss = 0.05885883\n",
            "Iteration 341, loss = 0.05875699\n",
            "Iteration 342, loss = 0.05865785\n",
            "Iteration 343, loss = 0.05855618\n",
            "Iteration 344, loss = 0.05845836\n",
            "Iteration 345, loss = 0.05835815\n",
            "Iteration 346, loss = 0.05825813\n",
            "Iteration 347, loss = 0.05815961\n",
            "Iteration 348, loss = 0.05806115\n",
            "Iteration 349, loss = 0.05796204\n",
            "Iteration 350, loss = 0.05786506\n",
            "Iteration 351, loss = 0.05776605\n",
            "Iteration 352, loss = 0.05767020\n",
            "Iteration 353, loss = 0.05757216\n",
            "Iteration 354, loss = 0.05747512\n",
            "Iteration 355, loss = 0.05737794\n",
            "Iteration 356, loss = 0.05728290\n",
            "Iteration 357, loss = 0.05718591\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(15, 10, 5), learning_rate='constant',\n",
              "              learning_rate_init=0.01, max_fun=15000, max_iter=400,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='sgd',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dPyLL90sEo-",
        "outputId": "6a125ef2-b0f9-4bc8-9b3a-a8819d52f5c7"
      },
      "source": [
        "accuracy = 0\n",
        "for i in range(len(X)):\n",
        "\tsample = X[i].reshape(1,-1) \n",
        "\tlabel = Y[i]\n",
        "\ty_pred = clf.predict(sample)\n",
        "\tif label == y_pred:\n",
        "\t\taccuracy+=1\n",
        "accuracy /= len(Y)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9733333333333334\n"
          ]
        }
      ]
    }
  ]
}